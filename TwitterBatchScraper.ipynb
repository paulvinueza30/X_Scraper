{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install ntscraper pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import time\n",
        "from typing import Iterable, List\n\n",
        "import pandas as pd\n",
        "from ntscraper import Nitter\n\n",
        "\n",
        "class TwitterBatchScraper:\n",
        "    \"\"\"Bulk scraper for public tweets via Nitter instances.\"\"\"\n\n",
        "    def __init__(self, target_accounts: Iterable[str], tweets_per_user: int = 20):\n",
        "        if not isinstance(target_accounts, Iterable):\n",
        "            raise TypeError(\"target_accounts must be an iterable of usernames\")\n",
        "        self.target_accounts: List[str] = [str(user).strip() for user in target_accounts if str(user).strip()]\n",
        "        if not self.target_accounts:\n",
        "            raise ValueError(\"At least one target account must be provided\")\n",
        "        if tweets_per_user <= 0:\n",
        "            raise ValueError(\"tweets_per_user must be a positive integer\")\n",
        "        self.tweets_per_user = int(tweets_per_user)\n",
        "        self.scraper = Nitter()\n",
        "        self.dataframe: pd.DataFrame | None = None\n\n",
        "    def _scrape_user(self, username: str) -> pd.DataFrame:\n",
        "        \"\"\"Scrape tweets for a single user, returning a DataFrame.\"\"\"\n",
        "        try:\n",
        "            results = self.scraper.get_tweets(username, mode=\"user\", number=self.tweets_per_user)\n",
        "        except Exception as exc:  # noqa: BLE001\n",
        "            print(f\"[ERROR] Failed to scrape {username}: {exc}\")\n",
        "            return pd.DataFrame()\n\n",
        "        tweets = []\n",
        "        for tweet in results.get(\"tweets\", []):\n",
        "            stats = tweet.get(\"stats\", {}) if isinstance(tweet, dict) else {}\n",
        "            tweets.append(\n",
        "                {\n",
        "                    \"source_user\": username,\n",
        "                    \"username\": tweet.get(\"user\", {}).get(\"username\") if isinstance(tweet, dict) else None,\n",
        "                    \"text\": tweet.get(\"text\") if isinstance(tweet, dict) else None,\n",
        "                    \"date\": tweet.get(\"date\") if isinstance(tweet, dict) else None,\n",
        "                    \"likes\": stats.get(\"likes\"),\n",
        "                    \"retweets\": stats.get(\"retweets\"),\n",
        "                    \"replies\": stats.get(\"replies\"),\n",
        "                    \"quotes\": stats.get(\"quotes\"),\n",
        "                }\n",
        "            )\n\n",
        "        return pd.DataFrame(tweets)\n\n",
        "    def scrape_all(self) -> pd.DataFrame:\n",
        "        \"\"\"Scrape tweets for all target accounts and aggregate into a DataFrame.\"\"\"\n",
        "        all_frames: list[pd.DataFrame] = []\n\n",
        "        for idx, user in enumerate(self.target_accounts, start=1):\n",
        "            print(f\"[{idx}/{len(self.target_accounts)}] Scraping @{user} (limit={self.tweets_per_user})\")\n",
        "            df_user = self._scrape_user(user)\n",
        "            if not df_user.empty:\n",
        "                all_frames.append(df_user)\n",
        "                print(f\"Collected {len(df_user)} tweets from @{user}\")\n",
        "            else:\n",
        "                print(f\"No tweets collected for @{user}\")\n\n",
        "            # Rate limiting between users\n",
        "            delay = random.uniform(2, 5)\n",
        "            time.sleep(delay)\n\n",
        "        if all_frames:\n",
        "            self.dataframe = pd.concat(all_frames, ignore_index=True)\n",
        "        else:\n",
        "            self.dataframe = pd.DataFrame(\n",
        "                columns=[\"source_user\", \"username\", \"text\", \"date\", \"likes\", \"retweets\", \"replies\", \"quotes\"]\n",
        "            )\n\n",
        "        return self.dataframe\n\n",
        "    def save_to_csv(self, filename: str = \"batch_scrape_results.csv\") -> None:\n",
        "        \"\"\"Persist the aggregated DataFrame to disk.\"\"\"\n",
        "        if self.dataframe is None:\n",
        "            raise ValueError(\"No data to save. Run scrape_all() first.\")\n",
        "        self.dataframe.to_csv(filename, index=False)\n",
        "        print(f\"Saved results to {filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample usage\n",
        "# Provide the list of target accounts and desired number of tweets per user\n",
        "sample_accounts = [\"SpaceX\", \"Tesla\"]\n",
        "scraper = TwitterBatchScraper(target_accounts=sample_accounts, tweets_per_user=5)\n",
        "result_df = scraper.scrape_all()\n",
        "result_df.head()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}